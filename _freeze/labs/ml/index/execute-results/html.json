{
  "hash": "fa2a261533bfc2fecde7cee3cc71179e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Mathematical statistics and machine learning in R\"\nauthor: \"Nikolay Oskolkov\"\ndescription: \"Coding artificial neural network from scratch in R.\"\nimage: \"assets/featured.webp\"\nformat: html\n---\n\n::: {.callout-note}\nIn this lab we will learn to code an Artificial Neural Network (ANN) in R both from scratch and using Keras / Tensorflow library. For simplicity, we consider a problem of linearly separable two classes of data points.\n:::\n\n::: {.callout-warning}\nSetting up keras properly might be tricky depending on your OS and setup. You will have to set up an environment using reticulate and install keras and tensorflow python packages.\n:::\n\nLoad the `keras` library and tensorflow in your environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate)\nlibrary(keras)\nreticulate::py_require(c(\"keras\",\"tensorflow\"))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# alternative solutions\n\n# reticulate::install_python()\n# keras::install_keras()\n# OR\n# keras::install_keras(method=\"conda\", envname=\"raukr-keras\")\n# OR\n# conda_create(\"raukr-keras\", packages=c(\"keras\", \"tensorflow\"), channel=\"conda-forge\", python_version=\"3.10\")\n# if using container\n# use_condaenv(\"raukr-keras\")\n```\n:::\n\n\n## Problem formulation\n\nLet us consider a simple problem of only **two features**, i.e. $X_1$ and $X_2$, and only **four statistical observations (data points)** that belong to two classes: **circles and crosses**. The four data points are schematically depicted below using $X_1$ vs. $X_2$ coordinate system. Obviously, the circles and crosses are separable with a **linear** decision boundary, i.e. hyperplane.\n\n![](assets/Problem.png)\n\nLet us implement the simplest possible feedforward / dense Artificial Neural Network (ANN) without hidden layers using Keras / Tensorflow library. Later, we will reproduce the results from Keras using from scratch coding ANN in R.\n\n![](assets/ANN_Scheme.png)\n\nThe architecture of the simplest ANN is displayed above, and includes two input nodes (two feature vectors $X_1$ and $X_2$) and one output node, where the two classes are coded in the following way: circles are codded as 0 and crosses as 1. The weights $w_1$ and $w_2$ of the edges of the ANN graph are the fitting parameters of the model.\n\n## Keras solution\n\nLet us first define the X matrix of the feature vectors and the y vector of the class labels:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- matrix(c(c(0, 0, 1, 1), c(0, 1, 0, 1)), ncol = 2)\nX\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    0    0\n[2,]    0    1\n[3,]    1    0\n[4,]    1    1\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- c(0, 0, 1, 1)\ny\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0 0 1 1\n```\n\n\n:::\n:::\n\n\nNow, let us define a sequential Keras model of the ANN corresponding to the scheme above and print the summary of the model. Here, we are going to use Sigmoid activation function on the output node because we have a binary classification problem.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nmodel <- keras_model_sequential(layers = list(\n  layer_input(shape = c(2)),\n  layer_dense(units = 1, activation = \"sigmoid\")\n))\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense (Dense)                      (None, 1)                       3           \n================================================================================\nTotal params: 3 (12.00 Byte)\nTrainable params: 3 (12.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n```\n\n\n:::\n:::\n\n\nNext, we are going to compile and fit the Keras ANN model. Again, for simplicity, we are going to use Mean Squared Error (MSE) loss function, and Stochastic Gradient Descent (SGD) as an optimization algorithm (with a high learning rate 0.1). The training will be for 3000 epochs, it should be enough for MSE to reach zero.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>%\n  compile(loss = \"mean_squared_error\", optimizer = optimizer_sgd(lr = 0.1))\nhistory <- model %>% fit(X, y, epochs = 3000)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(history$metrics$loss ~ seq(1:length(history$metrics$loss)),\n  xlab = \"Epochs\", ylab = \"Loss\", col = \"blue\", cex = 0.5\n)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\nFinally, we will make predictions on the same data set. Overfitting is not a concern here because we want to make sure that the model was capable of linearly separating the two classes of data points.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% predict(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1/1 - 0s - 30ms/epoch - 30ms/step\n          [,1]\n[1,] 0.3404378\n[2,] 0.3203223\n[3,] 0.7482035\n[4,] 0.7306861\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>%\n  predict(X) %>%\n  `>`(0.5) %>%\n  k_cast(\"int32\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1/1 - 0s - 7ms/epoch - 7ms/step\ntf.Tensor(\n[[0]\n [0]\n [1]\n [1]], shape=(4, 1), dtype=int32)\n```\n\n\n:::\n:::\n\n\nIt looks like the Keras model successfully can assign correct labels to the four data points.\n\n## Coding ANN from scratch in R\n\nNow we are going to implement the same ANN architecture from scratch in R. This will allow us to better understand the concepts like learning rate, gradient descent as well as to get an intuition of forward- and back-propagation. First of all, let us denote the sigmoid activation function on the output node as\n\n$$\\phi(s)=\\frac{1}{1+\\exp^{\\displaystyle -s}}$$\n\nThe beauty of this function is that it has a simple derivative that is expressed through the sigmoid function itself:\n\n$$\\phi^\\prime(s)=\\phi(s)\\left(1-\\phi(s)\\right)$$\n\nNext, the loss MSE function, i.e. the squared difference between the prediction y and the truth d, is given by the following simple equation:\n\n$$E(w_1,w_2)=\\frac{1}{2}\\sum_{i=1}^N\\left(d_i-y_i(w_1,w_2)\\right)^2; \\,\\,\\,\\,\\, y(w_1,w_2)=\\phi(w_1x_1+w_2x_2)$$\n\nFinally, the gradient descent update rule can be written as follows:\n\n$$w_{1,2}=w_{1,2}-\\mu\\frac{\\partial E}{\\partial w_{1,2}}$$\n\n$$\\frac{\\partial E}{\\partial w_{1,2}}=-(d-y)*y*(1-y)*x_{1,2}$$\n\nwhere $\\mu$ is a learning rate. Let us put it all together in a simple for-loop that updates the fitting parameters $w_1$ and $w_2$ via minimizing the mean squared error:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nphi <- function(x) {\n  return(1 / (1 + exp(-x)))\n}\n\nX <- matrix(c(c(0, 0, 1, 1), c(0, 1, 0, 1)), ncol = 2)\nd <- matrix(c(0, 0, 1, 1), ncol = 1)\n\nmu <- 0.1\nN_epochs <- 10000\nE <- vector()\nw <- matrix(c(0.1, 0.5), ncol = 1) # initialization of weights w1 and w2\nfor (epochs in 1:N_epochs)\n{\n  # Forward propagation\n  y <- phi(X %*% w - 3) # here for simplicity we use fixed bias = -3\n\n  # Backward propagation\n  E <- append(E, sum((d - y)^2))\n  dE_dw <- (d - y) * y * (1 - y)\n  w <- w + mu * (t(X) %*% dE_dw)\n}\nplot(E ~ seq(1:N_epochs), cex = 0.5, xlab = \"Epochs\", ylab = \"Error\", col = \"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\nThe mean squared error seems to be decreasing and reaching zero. Let us display the final y vector of predicted labels, it should be equal to the d vector of true labels.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           [,1]\n[1,] 0.04742587\n[2,] 0.03166204\n[3,] 0.98444378\n[4,] 0.97650411\n```\n\n\n:::\n:::\n\n\nIndeed, the predicted values of labels are very close to the true ones and similar to the ones obtained from Keras solution. Well done, we have successfully implemented an ANN from scratch in R!\n\n## Session\n\n<details>\n  <summary>Click here</summary>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.5\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Stockholm\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] keras_2.15.0      reticulate_1.42.0\n\nloaded via a namespace (and not attached):\n [1] cli_3.6.3         knitr_1.49        rlang_1.1.4       zeallot_0.2.0    \n [5] xfun_0.49         png_0.1-8         generics_0.1.3    jsonlite_1.8.9   \n [9] htmltools_0.5.8.1 rmarkdown_2.29    grid_4.4.2        evaluate_1.0.1   \n[13] tfruns_1.5.3      fastmap_1.2.0     base64enc_0.1-3   yaml_2.3.10      \n[17] lifecycle_1.0.4   whisker_0.4.1     compiler_4.4.2    codetools_0.2-20 \n[21] htmlwidgets_1.6.4 Rcpp_1.0.13-1     rstudioapi_0.17.1 lattice_0.22-6   \n[25] digest_0.6.37     R6_2.5.1          tensorflow_2.16.0 magrittr_2.0.3   \n[29] Matrix_1.7-1      tools_4.4.2      \n```\n\n\n:::\n:::\n\n\n</details>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}